_target_: lexikanon.pipe.tokenize.extract_tokens
tokenizer_config_name: simple
num_workers: 1
batched: true
batch_size: 1000
token_col: tokenizedText
extracted_col: extractedTokens
nouns_only: false
postags: null
stop_postags: null
strip_pos: null
postag_delim: null
postag_length: null
remove_columns: null
load_from_cache_file: true
num_heads: 1
num_tails: 1
verbose: false
