_target_: lexikanon.pipe.tokenize.tokenize_dataset
tokenizer_config_name: simple
num_workers: 1
batched: true
batch_size: 1000
text_col: bodyText
token_col: tokenizedText
remove_columns: null
load_from_cache_file: true
num_heads: 1
num_tails: 1
verbose: false
