_target_: lexikanon.pipes.tokenize.tokenize_dataset
tokenizer: simple
num_workers: 1
batched: true
batch_size: 1000
text_col: text
token_col: tokens
remove_columns: null
load_from_cache_file: true
num_heads: 1
num_tails: 1
verbose: false
