defaults:
  - __general_external_funcs__

run: lexikanon.pipe.tokenize.tokenize_dataset
run_with:
  tokenizer_config_name: simple
  num_workers: 1
  batched: true
  batch_size: 1000
  text_col: bodyText
  token_col: tokenizedText
  remove_columns:
  load_from_cache_file: true
  verbose: ${..verbose}
use_pipe_obj: true
return_pipe_obj: false
